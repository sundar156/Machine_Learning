{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "#Load the packages \n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from elapsedtimer import ElapsedTimer\n",
    "from pathlib import Path\n",
    "import keras\n",
    "from keras.layers import Embedding, Reshape,dot,Input,Dense,concatenate\n",
    "from keras.models import Sequential,Model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, Callback\n",
    "from keras.layers import Embedding, Reshape,dot,Input,Dense\n",
    "from keras.models import Sequential,Model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import h5py\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "\n",
    "def define_model(input_dim,dense_units=[25,50,100],architecture='concat'):\n",
    "    \n",
    "    user_id_1 = Input(shape=(input_dim,))\n",
    "    user_id_2 = Input(shape=(input_dim,))\n",
    "    layer_1 = Dense(dense_units[0],activation='relu')\n",
    "    layer_2 = Dense(dense_units[1],activation='relu')\n",
    "    layer_3 = Dense(dense_units[2],activation='relu')\n",
    "    x1 = layer_1(user_id_1)\n",
    "    x1 = layer_2(x1)\n",
    "    x1 = layer_3(x1)\n",
    "    x2 = layer_1(user_id_2)\n",
    "    x2 = layer_2(x2)\n",
    "    x2 = layer_3(x2)\n",
    "    if architecture == 'concat':\n",
    "        out = concatenate([x1,x2],axis=-1)\n",
    "        out = Dense(100,activation='relu')(out)\n",
    "        out = Dense(50,activation='relu')(out)\n",
    "    else:\n",
    "        out = dot([x1,x2],axes=1,normalize=False)\n",
    "        out= Reshape((1,))(out)\n",
    "\n",
    "    out = Dense(1,activation='sigmoid')(out)\n",
    "    model = Model(inputs=[user_id_1,user_id_2],outputs=out)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def data_creation(path,feat_path='train/user_features.csv',mode='train'):\n",
    "    # Joining the features with Train\n",
    "    train = pd.read_csv(path)\n",
    "    feat_df = pd.read_csv(feat_path)\n",
    "    df_merge_train = train.merge(feat_df,left_on=['node1_id'],right_on=['node_id'],how='left')\n",
    "    df_merge_train = df_merge_train.merge(feat_df,left_on=['node2_id'],right_on=['node_id'],how='left')\n",
    "    feat = ['f' + str(i) for i in range(1,14)]\n",
    "    feat_x = [f + '_x' for f in feat]\n",
    "    feat_y = [f + '_y' for f in feat]\n",
    "    X1 = df_merge_train[feat_x].values\n",
    "    X2 = df_merge_train[feat_y].values\n",
    "    if mode == 'train':\n",
    "        y = df_merge_train['is_chat'].values\n",
    "    else:\n",
    "        y = df_merge_train[['id']]\n",
    "    return X1,X2,y\n",
    "\n",
    "\n",
    "def train_val_split(X1,X2,y,test_frac=0.25):\n",
    "    X1_train,X1_val,X2_train,X2_val,y_train,y_val = train_test_split(X1,X2,y,test_size=test_frac)\n",
    "    return X1_train,X1_val,X2_train,X2_val,y_train,y_val\n",
    "\n",
    "def normalize(X1_train,X2_train,X1_val,X2_val):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X1_train)\n",
    "    X1_train = scaler.transform(X1_train)\n",
    "    X2_train = scaler.transform(X2_train)\n",
    "    X1_val = scaler.transform(X1_val)\n",
    "    X2_val = scaler.transform(X2_val)\n",
    "    return X1_train,X2_train,X1_val,X2_val,scaler\n",
    "\n",
    "\n",
    "def train(X1_train,X1_val,X2_train,X2_val,y_train,y_val,outdir,input_dim,dense_units,k=1,batch_size=1024,epochs=10,lr=0.0001,architecture='concat'):\n",
    "    \n",
    "    model = define_model(input_dim,dense_units,architecture)\n",
    "    adam = Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss=[\"binary_crossentropy\"],metrics=[auc])\n",
    "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_auc',patience=3,factor=0.90)\n",
    "    callbacks = [\n",
    "    EarlyStopping(monitor='val_auc', patience=10, mode='max', verbose=1),\n",
    "    CSVLogger('keras-5fold-run-01-v1-epochs_ib.log', separator=',', append=False),reduce_lr,\n",
    "    ModelCheckpoint('kera1-5fold-run-01-v1-fold-' + str('%02d' % (k + 1)) + '-run-' + str('%02d' % (1 + 1)) + '.check',\n",
    "                    monitor='val_auc', mode='max',\n",
    "                    save_best_only=True,\n",
    "                    verbose=1)]\n",
    "    model.fit([X1_train,X2_train],y_train, \n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,verbose=1,\n",
    "              validation_data=([X1_val,X2_val],y_val),\n",
    "              callbacks=callbacks,\n",
    "              class_weight={0:0.032,1:0.968})\n",
    "    model_name = 'kera1-5fold-run-01-v1-fold-' + str('%02d' % (k + 1)) + '-run-' + str('%02d' % (1 + 1)) + '.check'\n",
    "    del model\n",
    "    f = h5py.File(model_name, 'r+')\n",
    "    del f['optimizer_weights']\n",
    "    f.close()\n",
    "    model_final = keras.models.load_model(model_name,custom_objects={'auc': auc})\n",
    "    model_name1 = f'{outdir}model_bsize_{batch_size}_epochs_{epochs}_lr_{lr}_archi_{architecture}.h5'\n",
    "    model_final.save(model_name1)\n",
    "    print(f'Model saved to Destination {model_name1}')\n",
    "    return model_name1\n",
    "\n",
    "def process_main(path,feat_path,outdir,batch_size,epochs,lr,architecture):\n",
    "    with ElapsedTimer('Data Extract'):\n",
    "        X1,X2,y = data_creation(path,feat_path,'train')\n",
    "        print('Data Extracted..')\n",
    "    with ElapsedTimer('Train/Val split'):    \n",
    "        X1_train,X1_val,X2_train,X2_val,y_train,y_val = train_val_split(X1,X2,y,test_frac=0.3)\n",
    "        print('Train/val splits completed..') \n",
    "    with ElapsedTimer('Min Max Scaling'):\n",
    "        X1_train,X2_train,X1_val,X2_val,scaler = normalize(X1_train,X2_train,X1_val,X2_val)\n",
    "        joblib.dump(scaler,outdir + 'scaler.pkl')\n",
    "        print('Shape of X1_train:',X1_train.shape)\n",
    "        print('Shape of X2_train:',X2_train.shape)\n",
    "        print('Shape of X1_val:',X1_val.shape)\n",
    "        print('Shape of X2_val:',X2_val.shape)\n",
    "        print('Data normalized..model to train now')\n",
    "    with ElapsedTimer('Model Training'):\n",
    "        model_name = train(X1_train,X1_val,X2_train,X2_val,y_train,y_val,outdir,input_dim=13,\n",
    "                           dense_units=[100,200,100],k=1,batch_size=batch_size,\n",
    "                           epochs=epochs,lr=lr,architecture=architecture)\n",
    "    return model_name\n",
    "\n",
    "def inference(path,feat_path,model_path,scaler_path,outdir):\n",
    "    model = keras.models.load_model(model_path,custom_objects={'auc': auc})\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    with ElapsedTimer('Test Data Creation'):\n",
    "        X1,X2,df_test = data_creation(path,feat_path,'test')\n",
    "        X1 = scaler.transform(X1)\n",
    "        X2 = scaler.transform(X2)\n",
    "    with ElapsedTimer('Prediction'):    \n",
    "        out = model.predict([X1,X2],batch_size=10000)\n",
    "        df_test['is_chat'] = out[:,0]\n",
    "        submission_file = f\"{outdir}{model_path.split('/')[-1]}_submission.csv\"\n",
    "        df_test.to_csv(submission_file,index=False)\n",
    "        print(f'Submission file written to {submission_file}')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with ElapsedTimer('Training Pipeline'):\n",
    "        path = 'train/train.csv'\n",
    "        feat_path = 'train/user_features.csv'\n",
    "        outdir = '/home/santanu/hike/'\n",
    "        batch_size = 1024\n",
    "        epochs = 58\n",
    "        lr = 0.0001\n",
    "        architecture = 'concat'\n",
    "        model_name = process_main(path,feat_path,outdir,batch_size,epochs,lr,architecture)    \n",
    "    with ElapsedTimer('Inference Pipeline'):    \n",
    "        path = 'test.csv'\n",
    "        feat_path = 'train/user_features.csv'\n",
    "        outdir = '/home/santanu/hike/'\n",
    "        scaler_path = '/home/santanu/hike/scaler.pkl'\n",
    "        #model_path = '/home/santanu/hike/kera1-5fold-run-01-v1-fold-02-run-02.check'\n",
    "        model_path = model_name\n",
    "        inference(path,feat_path,model_path,scaler_path,outdir)\n",
    "\n",
    "\n",
    "              \n",
    "              \n",
    "              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
